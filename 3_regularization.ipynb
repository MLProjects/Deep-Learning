{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 28, 28) (438047,)\n",
      "Validation set (15586, 28, 28) (15586,)\n",
      "Test set (13645, 28, 28) (13645,)\n"
     ]
    }
   ],
   "source": [
    "#pickle_file = 'notMNIST.pickle'\n",
    "pickle_file = 'notMNIST_noDupNorOvlp.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 784) (438047, 10)\n",
      "Validation set (15586, 784) (15586, 10)\n",
      "Test set (13645, 784) (13645, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 1024\n",
    "batch_size = 512\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    \n",
    "  # Variables Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "  relu = tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  loss = loss + beta * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for validation \n",
    "  logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3506.561035\n",
      "Minibatch accuracy: 11.5%\n",
      "Validation accuracy: 33.9%\n",
      "Minibatch loss at step 500: 21.353649\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1000: 0.915069\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500: 0.679255\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2000: 0.781053\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2500: 0.683491\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 3000: 0.830468\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 3500: 0.833612\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 4000: 0.719391\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 4500: 0.794935\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 5000: 0.641867\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 5500: 0.769989\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 6000: 0.731598\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 6500: 0.730400\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 7000: 0.803917\n",
      "Minibatch accuracy: 82.4%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 7500: 0.626684\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 8000: 0.741604\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 8500: 0.638537\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 9000: 0.722373\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 9500: 0.749563\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 10000: 0.808721\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3448.093750\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 37.5%\n",
      "Minibatch loss at step 500: 21.140400\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 0.656202\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1500: 0.479717\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2000: 0.461609\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500: 0.462079\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 3000: 0.454983\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 3500: 0.447627\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 4000: 0.452734\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 4500: 0.447139\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 5000: 0.442182\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 5500: 0.446830\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 6000: 0.443198\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 6500: 0.438112\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 7000: 0.445897\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 7500: 0.441202\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 8000: 0.437793\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 8500: 0.445442\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 9000: 0.440815\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 9500: 0.435203\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 10000: 0.442636\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 86.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = batch_size * (step % 3)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 1024\n",
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    \n",
    "  # Variables Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits_1 = tf.add(tf.matmul(tf_train_dataset, weights_1), biases_1)\n",
    "  relu = tf.nn.relu(logits_1)\n",
    "  relu_drop = tf.nn.dropout(relu, 0.5)\n",
    "  logits_2 = tf.add(tf.matmul(relu_drop, weights_2), biases_2)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  loss = loss + beta * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for validation \n",
    "  logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3580.060059\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 27.7%\n",
      "Minibatch loss at step 500: 21.464748\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1000: 0.875292\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.998776\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 0.893566\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2500: 0.771773\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.658304\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3500: 0.814337\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 4000: 0.898360\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 4500: 0.866189\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 5000: 0.916550\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 86.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3404.252686\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 29.8%\n",
      "Minibatch loss at step 500: 21.097956\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 1000: 0.453407\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1500: 0.289589\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 2000: 0.283881\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 2500: 0.273648\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3000: 0.270465\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3500: 0.273427\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 4000: 0.267368\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4500: 0.265939\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 5000: 0.270107\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Test accuracy: 83.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = batch_size * (step % 3)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n",
    "The proposed NN design will be:\n",
    "Layer1: 1024 nodes\n",
    "Layer2: 512 nodes\n",
    "Layer3: 256 nodes\n",
    "Layer4: 128 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "\n",
    "# NN architecture\n",
    "num_nodes_1 = 2400\n",
    "num_nodes_2 = 1200\n",
    "num_nodes_3 = 600\n",
    "num_nodes_4 = 300\n",
    "\n",
    "batch_size = 1000\n",
    "beta = 0.0005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes_1], stddev=m.sqrt(2.0/(image_size*image_size))))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes_1]))\n",
    "    \n",
    "  # Variables Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_1, num_nodes_2],stddev=m.sqrt(2.0/num_nodes_1)))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_nodes_2]))\n",
    "\n",
    "  # Variables Layer 3\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_2, num_nodes_3],stddev=m.sqrt(2.0/num_nodes_2)))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_nodes_3]))\n",
    "\n",
    "  # Variables Layer 4\n",
    "  weights_4 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_3, num_nodes_4],stddev=m.sqrt(2.0/num_nodes_3)))\n",
    "  biases_4 = tf.Variable(tf.zeros([num_nodes_4]))\n",
    "    \n",
    "  # Variables Layer 5\n",
    "  weights_5 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_4, num_labels],stddev=m.sqrt(2.0/num_nodes_4)))\n",
    "  biases_5 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits_1 = tf.add(tf.matmul(tf_train_dataset, weights_1), biases_1)\n",
    "  relu_2 = tf.nn.dropout(tf.nn.relu(logits_1), 0.5)\n",
    "  logits_2 = tf.add(tf.matmul(relu_2, weights_2), biases_2)\n",
    "  relu_3 = tf.nn.dropout(tf.nn.relu(logits_2), 0.5)\n",
    "  logits_3 = tf.add(tf.matmul(relu_3, weights_3), biases_3)\n",
    "  relu_4 = tf.nn.dropout(tf.nn.relu(logits_3), 0.5)\n",
    "  logits_4 = tf.add(tf.matmul(relu_4, weights_4), biases_4)\n",
    "  relu_5 = tf.nn.dropout(tf.nn.relu(logits_4), 0.5)\n",
    "  logits_5 = tf.add(tf.matmul(relu_5, weights_5), biases_5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_5))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) \\\n",
    "    + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4) + tf.nn.l2_loss(weights_5)\n",
    "  loss = tf.reduce_mean(loss + beta * regularization)\n",
    "  \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 24000, 0.99)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits_5)\n",
    "    \n",
    "  # Predictions for validation \n",
    "  logits_1 = tf.add(tf.matmul(tf_valid_dataset, weights_1), biases_1)\n",
    "  relu_2 = tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_2, weights_2) + biases_2\n",
    "  relu_3 = tf.nn.relu(logits_2)\n",
    "  logits_3 = tf.matmul(relu_3, weights_3) + biases_3\n",
    "  relu_4 = tf.nn.relu(logits_3)\n",
    "  logits_4 = tf.matmul(relu_4, weights_4) + biases_4\n",
    "  relu_5 = tf.nn.relu(logits_4)\n",
    "  logits_5 = tf.matmul(relu_5, weights_5) + biases_5\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(logits_5)\n",
    "    \n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_2 = tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_2, weights_2) + biases_2\n",
    "  relu_3 = tf.nn.relu(logits_2)\n",
    "  logits_3 = tf.matmul(relu_3, weights_3) + biases_3\n",
    "  relu_4 = tf.nn.relu(logits_3)\n",
    "  logits_4 = tf.matmul(relu_4, weights_4) + biases_4\n",
    "  relu_5 = tf.nn.relu(logits_4)\n",
    "  logits_5 = tf.matmul(relu_5, weights_5) + biases_5\n",
    "    \n",
    "  test_prediction =  tf.nn.softmax(logits_5)\n",
    "\n",
    "  # Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.465448\n",
      "Minibatch accuracy: 12.0%\n",
      "Validation accuracy: 12.7%\n",
      "Minibatch loss at step 500: 2.105588\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.3%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f433e973990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 25001\n",
    "\n",
    "#Plots\n",
    "p_freq = 100\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    opt, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    #Plot variables\n",
    "    if (step % p_freq == 0):\n",
    "       losses.append(l)\n",
    "       accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "       rates.append(learning_rate.eval())\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  save_path = saver.save(session, \"/notebooks/weights/model.ckpt\")\n",
    "  print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps, p_freq), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, num_steps, p_freq), accuracies)\n",
    "ax2.set_ylabel(\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, num_steps, p_freq), rates)\n",
    "ax3.set_ylabel(\"Learning rate\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "ax3.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/weights/model.ckpt\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"/notebooks/weights/model.ckpt\")\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
