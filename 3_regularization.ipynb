{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 28, 28) (438047,)\n",
      "Validation set (15586, 28, 28) (15586,)\n",
      "Test set (13645, 28, 28) (13645,)\n"
     ]
    }
   ],
   "source": [
    "#pickle_file = 'notMNIST.pickle'\n",
    "pickle_file = 'notMNIST_noDupNorOvlp.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 784) (438047, 10)\n",
      "Validation set (15586, 784) (15586, 10)\n",
      "Test set (13645, 784) (13645, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 1024\n",
    "batch_size = 512\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    \n",
    "  # Variables Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "  relu = tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  loss = loss + beta * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for validation \n",
    "  logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3506.561035\n",
      "Minibatch accuracy: 11.5%\n",
      "Validation accuracy: 33.9%\n",
      "Minibatch loss at step 500: 21.353649\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1000: 0.915069\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500: 0.679255\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2000: 0.781053\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2500: 0.683491\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 3000: 0.830468\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 3500: 0.833612\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 4000: 0.719391\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 4500: 0.794935\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 5000: 0.641867\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 5500: 0.769989\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 6000: 0.731598\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 6500: 0.730400\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 7000: 0.803917\n",
      "Minibatch accuracy: 82.4%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 7500: 0.626684\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 8000: 0.741604\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 8500: 0.638537\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 9000: 0.722373\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 9500: 0.749563\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 10000: 0.808721\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3448.093750\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 37.5%\n",
      "Minibatch loss at step 500: 21.140400\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 0.656202\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1500: 0.479717\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2000: 0.461609\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500: 0.462079\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 3000: 0.454983\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 3500: 0.447627\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 4000: 0.452734\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 4500: 0.447139\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 5000: 0.442182\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 5500: 0.446830\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 6000: 0.443198\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 6500: 0.438112\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 7000: 0.445897\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 7500: 0.441202\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 8000: 0.437793\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 8500: 0.445442\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 9000: 0.440815\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 9500: 0.435203\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 10000: 0.442636\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 86.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = batch_size * (step % 3)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 1024\n",
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    \n",
    "  # Variables Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits_1 = tf.add(tf.matmul(tf_train_dataset, weights_1), biases_1)\n",
    "  relu = tf.nn.relu(logits_1)\n",
    "  relu_drop = tf.nn.dropout(relu, 0.5)\n",
    "  logits_2 = tf.add(tf.matmul(relu_drop, weights_2), biases_2)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  loss = loss + beta * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for validation \n",
    "  logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "  test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3580.060059\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 27.7%\n",
      "Minibatch loss at step 500: 21.464748\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1000: 0.875292\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.998776\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 0.893566\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2500: 0.771773\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.658304\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3500: 0.814337\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 4000: 0.898360\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 4500: 0.866189\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 5000: 0.916550\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 86.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3404.252686\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 29.8%\n",
      "Minibatch loss at step 500: 21.097956\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 1000: 0.453407\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1500: 0.289589\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 2000: 0.283881\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 2500: 0.273648\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3000: 0.270465\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3500: 0.273427\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 4000: 0.267368\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4500: 0.265939\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 5000: 0.270107\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Test accuracy: 83.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = batch_size * (step % 3)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n",
    "The proposed NN design will be:\n",
    "Layer1: 1024 nodes\n",
    "Layer2: 512 nodes\n",
    "Layer3: 256 nodes\n",
    "Layer4: 128 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-6fa42705158c>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "\n",
    "# NN architecture\n",
    "num_nodes_1 = 2400\n",
    "num_nodes_2 = 1200\n",
    "num_nodes_3 = 600\n",
    "num_nodes_4 = 300\n",
    "\n",
    "batch_size = 1000\n",
    "beta = 0.0005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes_1], stddev=m.sqrt(2.0/(image_size*image_size))))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes_1]))\n",
    "    \n",
    "  # Variables Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_1, num_nodes_2],stddev=m.sqrt(2.0/num_nodes_1)))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_nodes_2]))\n",
    "\n",
    "  # Variables Layer 3\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_2, num_nodes_3],stddev=m.sqrt(2.0/num_nodes_2)))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_nodes_3]))\n",
    "\n",
    "  # Variables Layer 4\n",
    "  weights_4 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_3, num_nodes_4],stddev=m.sqrt(2.0/num_nodes_3)))\n",
    "  biases_4 = tf.Variable(tf.zeros([num_nodes_4]))\n",
    "    \n",
    "  # Variables Layer 5\n",
    "  weights_5 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes_4, num_labels],stddev=m.sqrt(2.0/num_nodes_4)))\n",
    "  biases_5 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits_1 = tf.add(tf.matmul(tf_train_dataset, weights_1), biases_1)\n",
    "  relu_2 = tf.nn.dropout(tf.nn.relu(logits_1), 0.5)\n",
    "  logits_2 = tf.add(tf.matmul(relu_2, weights_2), biases_2)\n",
    "  relu_3 = tf.nn.dropout(tf.nn.relu(logits_2), 0.5)\n",
    "  logits_3 = tf.add(tf.matmul(relu_3, weights_3), biases_3)\n",
    "  relu_4 = tf.nn.dropout(tf.nn.relu(logits_3), 0.5)\n",
    "  logits_4 = tf.add(tf.matmul(relu_4, weights_4), biases_4)\n",
    "  relu_5 = tf.nn.dropout(tf.nn.relu(logits_4), 0.5)\n",
    "  logits_5 = tf.add(tf.matmul(relu_5, weights_5), biases_5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_5))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) \\\n",
    "    + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4) + tf.nn.l2_loss(weights_5)\n",
    "  loss = tf.reduce_mean(loss + beta * regularization)\n",
    "  \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 24000, 0.99)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits_5)\n",
    "    \n",
    "  # Predictions for validation \n",
    "  logits_1 = tf.add(tf.matmul(tf_valid_dataset, weights_1), biases_1)\n",
    "  relu_2 = tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_2, weights_2) + biases_2\n",
    "  relu_3 = tf.nn.relu(logits_2)\n",
    "  logits_3 = tf.matmul(relu_3, weights_3) + biases_3\n",
    "  relu_4 = tf.nn.relu(logits_3)\n",
    "  logits_4 = tf.matmul(relu_4, weights_4) + biases_4\n",
    "  relu_5 = tf.nn.relu(logits_4)\n",
    "  logits_5 = tf.matmul(relu_5, weights_5) + biases_5\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(logits_5)\n",
    "    \n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_2 = tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_2, weights_2) + biases_2\n",
    "  relu_3 = tf.nn.relu(logits_2)\n",
    "  logits_3 = tf.matmul(relu_3, weights_3) + biases_3\n",
    "  relu_4 = tf.nn.relu(logits_3)\n",
    "  logits_4 = tf.matmul(relu_4, weights_4) + biases_4\n",
    "  relu_5 = tf.nn.relu(logits_4)\n",
    "  logits_5 = tf.matmul(relu_5, weights_5) + biases_5\n",
    "    \n",
    "  test_prediction =  tf.nn.softmax(logits_5)\n",
    "\n",
    "  # Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.738825\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 12.5%\n",
      "Minibatch loss at step 500: 1.886789\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 1.561030\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1500: 1.288349\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2000: 1.098930\n",
      "Minibatch accuracy: 87.4%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 2500: 0.890091\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.804554\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 3500: 0.771597\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 4000: 0.652124\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 4500: 0.589847\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5000: 0.598455\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5500: 0.566183\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6000: 0.455240\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.452591\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 7000: 0.538433\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 7500: 0.466923\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.469284\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8500: 0.470866\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9000: 0.468169\n",
      "Minibatch accuracy: 90.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9500: 0.424316\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 10000: 0.439377\n",
      "Minibatch accuracy: 90.7%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10500: 0.429198\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 11000: 0.428965\n",
      "Minibatch accuracy: 90.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11500: 0.433873\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 12000: 0.430070\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 12500: 0.470547\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 13000: 0.411580\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13500: 0.412871\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 14000: 0.452881\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 14500: 0.447251\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 15000: 0.468760\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 15500: 0.420691\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 16000: 0.367817\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 16500: 0.409318\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 17000: 0.401958\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 17500: 0.453342\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 18000: 0.422352\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 18500: 0.518173\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 19000: 0.408717\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 19500: 0.362841\n",
      "Minibatch accuracy: 93.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 20000: 0.381051\n",
      "Minibatch accuracy: 92.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 20500: 0.423451\n",
      "Minibatch accuracy: 91.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 21000: 0.405761\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 21500: 0.391201\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 22000: 0.445781\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 22500: 0.403739\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 23000: 0.417908\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 23500: 0.427952\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 24000: 0.400377\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 24500: 0.416099\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 25000: 0.433200\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 91.1%\n",
      "Test accuracy: 96.2%\n",
      "Model saved in path: /notebooks/weights/model.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5114f87f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 25001\n",
    "\n",
    "#Plots\n",
    "p_freq = 100\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print('Offset in iteration '+ str(step) + ' is: '+ str(offset))\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    opt, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    #Plot variables\n",
    "    if (step % p_freq == 0):\n",
    "       losses.append(l)\n",
    "       accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "       rates.append(learning_rate.eval())\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  save_path = saver.save(session, \"/notebooks/weights/model.ckpt\")\n",
    "  print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps, p_freq), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, num_steps, p_freq), accuracies)\n",
    "ax2.set_ylabel(\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, num_steps, p_freq), rates)\n",
    "ax3.set_ylabel(\"Learning rate\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "ax3.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/weights/model.ckpt\n",
      "Test accuracy: 96.2%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"/notebooks/weights/model.ckpt\")\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
