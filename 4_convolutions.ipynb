{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438087, 28, 28) (438087,)\n",
      "Validation set (15513, 28, 28) (15513,)\n",
      "Test set (13649, 28, 28) (13649,)\n"
     ]
    }
   ],
   "source": [
    "#pickle_file = 'notMNIST.pickle'\n",
    "pickle_file = 'notMNIST_noDupNorOvlp.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438087, 28, 28, 1) (438087, 10)\n",
      "Validation set (15513, 28, 28, 1) (15513, 10)\n",
      "Test set (13649, 28, 28, 1) (13649, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "#Plots\n",
    "p_freq = 10\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #Plot variables\n",
    "    if (step % p_freq == 0):\n",
    "       losses.append(l)\n",
    "       accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "       rates.append(0.05)\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps, p_freq), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, num_steps, p_freq), accuracies)\n",
    "ax2.set_ylabel(\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, num_steps, p_freq), rates)\n",
    "ax3.set_ylabel(\"Learning rate\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "ax3.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Convolution 1 Layer\n",
    "  # Input channels: num_channels = 1\n",
    "  # Output channels: depth = 16\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  # Convolution 2 Layer\n",
    "  # Input channels: depth = 16\n",
    "  # Output channels: depth = 16\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "  # Fully Connected Layer (Densely Connected Layer)\n",
    "  # Use neurons to allow processing of entire image\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "  # Readout layer: Softmax Layer\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # ksize is kernel size = [1, 2, 2, 1]\n",
    "    padd1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv2 = tf.nn.conv2d(padd1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    padd2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    shape = padd2.get_shape().as_list()\n",
    "    reshape = tf.reshape(padd2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "#Plots\n",
    "p_freq = 10\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #Plot variables\n",
    "    if (step % p_freq == 0):\n",
    "       losses.append(l)\n",
    "       accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "       rates.append(0.05)\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps, p_freq), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, num_steps, p_freq), accuracies)\n",
    "ax2.set_ylabel(\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, num_steps, p_freq), rates)\n",
    "ax3.set_ylabel(\"Learning rate\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "ax3.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "patch_size1 = 5\n",
    "patch_size2 = 3\n",
    "depth1 = 6\n",
    "depth2 = 12\n",
    "num_hidden_3 = 256\n",
    "beta = 0.0\n",
    "num_steps = 501\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Convolution 1 Layer\n",
    "  # Input channels: num_channels = 1\n",
    "  # Output channels: depth = 16\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size1, patch_size1, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "  # Convolution 2 Layer\n",
    "  # Input channels: depth = 16\n",
    "  # Output channels: depth = 16\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size2, patch_size2, depth1, depth2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "    \n",
    "  # Fully Connected Layer (Densely Connected Layer)\n",
    "  # Use neurons to allow processing of entire image\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [6*6*12, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "  # Readout layer: Softmax Layer\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    hidden1 = tf.nn.dropout(tf.nn.relu(conv1 + layer1_biases), 0.5)\n",
    "    # ksize is kernel size = [1, 2, 2, 1]\n",
    "    padd1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv2 = tf.nn.conv2d(padd1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.dropout(tf.nn.relu(conv2 + layer2_biases), 0.5)\n",
    "    padd2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    shape = padd2.get_shape().as_list()\n",
    "    reshape = tf.reshape(padd2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),0.5)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights) \\\n",
    "    + tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights)\n",
    "  loss = tf.reduce_mean(loss + beta * regularization)\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step, 2500, 0.1)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots\n",
    "p_freq = 2\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #Plot variables\n",
    "    if (step % p_freq == 0):\n",
    "       losses.append(l)\n",
    "       accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "       rates.append(learning_rate.eval())\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps, p_freq), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, num_steps, p_freq), accuracies)\n",
    "ax2.set_ylabel(\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, num_steps, p_freq), rates)\n",
    "ax3.set_ylabel(\"Learning rate\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "ax3.set_ylim([0, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1025\n",
    "patch_size1 = 9\n",
    "patch_size2 = 7\n",
    "depth1 = 25\n",
    "depth2 = 80\n",
    "num_hidden1 = 1728\n",
    "num_hidden2 = 864\n",
    "num_hidden3 = 200\n",
    "beta = 0.0001\n",
    "num_steps = 1001\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Convolution 1 Layer\n",
    "  # Input channels: num_channels = 1\n",
    "  # Output channels: depth = 16\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size1, patch_size1, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "  # Convolution 2 Layer\n",
    "  # Input channels: depth = 16\n",
    "  # Output channels: depth = 16\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size2, patch_size2, depth1, depth2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "    \n",
    "  # Fully Connected Layer (Densely Connected Layer)\n",
    "  # Use neurons to allow processing of entire image\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [7*7*depth2, num_hidden1], stddev=0.01))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden1, num_hidden2], stddev=0.01))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "    \n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden2, num_hidden3], stddev=0.01))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "    \n",
    "  # Readout layer: Softmax Layer\n",
    "  layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden3, num_labels], stddev=0.01))\n",
    "  layer6_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.dropout(tf.nn.relu(conv1 + layer1_biases), 0.75)\n",
    "    # ksize is kernel size = [1, 2, 2, 1]\n",
    "    padd1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv2 = tf.nn.conv2d(padd1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.dropout(tf.nn.relu(conv2 + layer2_biases), 0.75)\n",
    "    padd2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    shape = padd2.get_shape().as_list()\n",
    "    reshape = tf.reshape(padd2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden1 = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),0.75)\n",
    "    hidden2 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden1, layer4_weights) + layer4_biases),0.75)\n",
    "    hidden3 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden2, layer5_weights) + layer5_biases),0.9)\n",
    "    return tf.matmul(hidden3, layer6_weights) + layer6_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "  regularization = tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights) \\\n",
    "    + tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer5_weights)\\\n",
    "    + tf.nn.l2_loss(layer6_weights)\n",
    "  loss = tf.reduce_mean(loss + beta * regularization)\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step, 2000, 0.1)\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.384114\n",
      "Minibatch accuracy: 9.1%\n",
      "Validation accuracy: 10.5%\n",
      "Minibatch loss at step 50: 0.656428\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 100: 0.619350\n",
      "Minibatch accuracy: 82.1%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 150: 0.502779\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 200: 0.430326\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 250: 0.438589\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 300: 0.383042\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 350: 0.387351\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 400: 0.334925\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 450: 0.438181\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 500: 0.412943\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 550: 0.382585\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 600: 0.416334\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 650: 0.377667\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 700: 0.365014\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 750: 0.357128\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 800: 0.329514\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 850: 0.363445\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 900: 0.397000\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 950: 0.402879\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 1000: 0.305667\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 95.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAEKCAYAAACmO6mFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmoiIiEiQUQNNREREJMj8P2VYdcxt4UJfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd96809ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plots\n",
    "p_freq = 10\n",
    "losses = []\n",
    "accuracies = []\n",
    "rates = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #Plot variables\n",
    "    if (step % p_freq == 0):\n",
    "       losses.append(l)\n",
    "       accuracies.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "       rates.append(learning_rate.eval())\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "# Show the results.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.8)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps, p_freq), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "ax2.plot(range(0, num_steps, p_freq), accuracies)\n",
    "ax2.set_ylabel(\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(range(0, num_steps, p_freq), rates)\n",
    "ax3.set_ylabel(\"Learning rate\")\n",
    "ax3.set_xlabel(\"Training steps\")\n",
    "ax3.set_ylim([0, 0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
